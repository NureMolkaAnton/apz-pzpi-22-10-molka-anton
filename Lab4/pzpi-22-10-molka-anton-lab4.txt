Харківський національний університет радіоелектроніки

Кафедра програмної інженерії






Звіт
З Лабораторної роботи № 4
«Масштабування бекенда»

з дисципліни «Архітектура програмного забезпечення» 





Виконав:             		Перевірив:
студент гр. ПЗПІ 22-10  	асистент каф. ПІ
Молька Антон Васильович		Дашенков  Д. О.




Харків 2025
 
1. Опис стратегії масштабування системи
Для забезпечення продуктивності та надійності системи API обрано стратегією горизонтальне масштабування. Ця стратегія реалізується шляхом запуску декількох ідентичних екземплярів сервісу API в середовищі оркестрації контейнерів Kubernetes. Вхідні запити розподіляються між активними екземплярами за допомогою механізмів балансування навантаження. Такий підхід дозволяє збільшувати пропускну здатність системи та її відмовостійкість шляхом динамічного або ручного коригування кількості працюючих реплік відповідно до поточного навантаження. На поточному етапі використовується ручне горизонтальне масштабування, з можливістю подальшого впровадження автоматичного масштабування за допомогою Horizontal Pod Autoscaler.

2. Опис технічних рішень які роблять масштабування можливим;
Технічну реалізацію масштабування забезпечують декілька ключових рішень. По-перше, сервіс API, розроблений на Qt C++, контейнеризований за допомогою Docker, що гарантує ізоляцію залежностей та консистентність середовища виконання. Dockerfile описує процес створення образу, включаючи всі необхідні бібліотеки Qt та системні компоненти. По-друге, для оркестрації контейнерів використовується Kubernetes. Ресурс Deployment керує життєвим циклом API, підтримуючи задану кількість реплік. Кожен екземпляр API працює в окремому Pod'і. Доступ до групи Pod'ів та балансування навантаження між ними здійснюється через Kubernetes Service, у даній реалізації типу NodePort. Важливим аспектом є прагнення до stateless архітектури API, де стан сесії виноситься у зовнішні сховища, такі як база даних PostgreSQL та MQTT брокер Mosquitto, які розгорнуті як окремі сервіси. Це дозволяє будь-якому екземпляру API обробляти запити незалежно. Збірка та запуск проєкту автоматизована за допомогою CMake та скрипта apibuilder.sh.

 
3. Аналіз вузьких місць — які саме ресурси вичерпуються першими при навантаженні
Аналіз потенційних вузьких місць при горизонтальному масштабуванні API показав, що обмежуючими факторами можуть стати залежні сервіси. База даних PostgreSQL може вичерпати ресурси CPU, дискового вводу/виводу або досягти ліміту з'єднань при зростанні кількості запитів від масштабованих екземплярів API. Неоптимізовані SQL-запити та відсутність індексів посилюють цю проблему. MQTT брокер Mosquitto також може стати вузьким місцем при високій частоті повідомлень або великій кількості підключень, вичерпуючи CPU, мережеву пропускну здатність або пам'ять. Мережева інфраструктура Kubernetes, включаючи Service, kube-proxy та CNI, також має свою межу пропускної здатності. Самі Pod'и API, хоча й масштабуються, можуть досягати індивідуальних лімітів CPU та пам'яті, якщо запити є надто ресурсомісткими або якщо в коді API присутні витоки ресурсів чи неефективні алгоритми. Блокуючі операції або синхронізовані блоки коду в API можуть обмежувати лінійність приросту продуктивності від масштабування. Типовим порядком вичерпання ресурсів є: спочатку база даних, потім ефективність коду самого API, далі MQTT брокер та мережеві компоненти.
 
4. Опис навантажувальних тестів з ілюстрацією зв’язку між масштабуванням і
витримуваним навантаженням.
Для демонстрації зв'язку між масштабуванням і витримуваним навантаженням було проведено навантажувальне тестування за допомогою інструменту Locust. Сценарій тесту імітував поведінку віртуальних користувачів, які надсилали GET-запити до визначеного ендпоінту API, розгорнутого в локальному кластері Kubernetes та доступного через Service типу NodePort. Тестування проводилося у два етапи. На першому етапі API працював з однією реплікою. Було визначено максимальний стабільний показник запитів на секунду (RPS) у 290 запитів/сек при 390 користувачах та середньому часі відповіді 52 мс, за умови, що 95-й перцентиль часу відповіді не перевищував 320 мс, а помилоки були відсутні взагалі. На другому етапі кількість реплік API було збільшено до двох. Повторне тестування з аналогічним профілем навантаження показало, що система змогла витримати максимальний стабільний RPS на рівні 480 запитів/сек при 570 користувачах. Середній час відповіді при цьому майже не змінився, а кількість помилок залишалася мінімальною. Таким чином, збільшення кількості реплік API удвічі призвело до зростання витримуваного навантаження приблизно майже до 190%, що підтверджує ефективність обраної стратегії горизонтального масштабування.

 
Рисунок 1 – Тестування з однією реплікою API

 
Рисунок 2 – Тестування з двома репліками API 
 
ДОДАТОК А
Посилання на репозиторій проєкту на GitHub: https://github.com/NureMolkaAnton/apz-pzpi-22-10-molka-anton/tree/main/Lab4 
